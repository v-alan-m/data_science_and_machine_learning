{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "- Calculate the regression line by drawing a line that's as close to every data point as possible\n",
    "    - The **Least Sqaures Method** is used, and only measures the closeness in the **Up** and **Down** direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance and Cross Validation\n",
    "- Fundamental topic of understanding you models performance\n",
    "\n",
    "<br></br>\n",
    "## Bias-Variance\n",
    "- A point within the model where adding model complexity is only generated unwanted noise within the data\n",
    "- The **training error** goes down but the **test error** goes up\n",
    "- The model after the **bias-variance** trade-off begins to overfit\n",
    "\n",
    "<br></br>\n",
    "<img src='pics/bias-variance_trade-off.png'>\n",
    "\n",
    "<br></br>\n",
    "- If complexity is continually added to a model it will overfit to the training data\n",
    "    - Making it less usefull when applied to the untrained data, such as the test set\n",
    "\n",
    "<img src='pics/b_v_t_o_overfit.png'>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<img src='pics/b_v_flexibility.png'>\n",
    "\n",
    "<br></br>\n",
    "- **Flexibility:** Complexity of the model, i.e. the polynomial level of a regression fit (how many independent variables are being account for)\n",
    "- **Mean Squared Error**: The error metric\n",
    "\n",
    "<br></br>\n",
    "- The goal is to balance the **bias** and **variance** of your model, so that it has the lowest error\n",
    "    - Find training-data-point that closest to the mininium-error-point on the test-data\n",
    "    - Find the ideal number of variables for analysis\n",
    "        - In this example it is the blue dot, within the middle figure\n",
    "            - The blue dot is the quadratic fit\n",
    "\n",
    "<img src='pics/b_v_over_under_fit.png'>\n",
    "\n",
    "<br></br>\n",
    "- *Going left*: Higher bias, lower variance\n",
    "- *Going right*: Lower bias, higher variance\n",
    "- **Underfitting**: Not enough data has been included in the model, as the error could be reduced\n",
    "- **Overfitting**: Too much data has been included in the model, as the error could be reduced\n",
    "\n",
    "<br></br>\n",
    "- Pick a point as a **bias-variance trade-off**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "- Trying to predict the probability of discrete categories\n",
    "\n",
    "<br></br>\n",
    "- A method for classification\n",
    "    - Spam and real emails\n",
    "    - Loan default (yes/no)\n",
    "    - Diesease diagnosis (have/do not have)\n",
    "        - These are examples of **binary classification**, aka there are 2 classes\n",
    "            - Typically **0** and **1**\n",
    "            \n",
    "<br></br>\n",
    "- A linear regression will give a bad fit for **binary classification** models\n",
    "    - It would predict for values below zero\n",
    "        - Transofrmed into a **logistic regression** curve is used as it will only fit data betweem **0** and **1** \n",
    "\n",
    "<img src='pics/log_regress_curve.png'>\n",
    "\n",
    "<br></br>\n",
    "- The shape of the logistic regression curve is called the **Signoid**\n",
    "    - It will take in any value and only output a value between 0 and 1\n",
    "\n",
    "<img src='pics/log_reg_signoid.png'>\n",
    "\n",
    "<br></br>\n",
    "-  The *linear model* is transformed into the *logistic model*\n",
    "    - So that all the **output values** range from **0** to **1**\n",
    "\n",
    "<img src='pics/log_reg_transform.png'>\n",
    "\n",
    "<br></br>\n",
    "-  A line can be draw half way in the probability\n",
    "    - Anything **below** the line is **class 0**\n",
    "    - Anything **above** the line is **class 1**\n",
    "\n",
    "<img src='pics/log_reg_class_line.png'>\n",
    "\n",
    "<br></br>\n",
    "- Train the logistics model then test it\n",
    "    - A **confusion matrix** can be used to evaluate classification models\n",
    "\n",
    "<img src='pics/log_reg_confusion.png'>\n",
    "\n",
    "**Basic terminolgy:**\n",
    "- True positives (TP)\n",
    "- True negatives (TN)\n",
    "- False positives (FP): **Type 1 error**\n",
    "- True negatives (FN): **Type 2 error**\n",
    "\n",
    "<br></br>\n",
    "**Accuracy rate:**\n",
    "- How often is the value correct\n",
    "    - (TP + TN) / total = 150 / 165 = 0.91 = 91%\n",
    "\n",
    "<br></br>\n",
    "**Misclassification rate:**\n",
    "- How often is the value wrong\n",
    "    - (FP + FN) / total = 15 / 165 = 0.09 = 9%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbours (KNN)\n",
    "- Use for classification problems\n",
    "- When two or more classes have continuous parameters\n",
    "    - E.g. The hieght and weight of dogs and horses\n",
    "\n",
    "<img src='pics/knn_and_pre.png'>\n",
    "\n",
    "- Predict whether a data point represents a horse or a dog based on the height and weight\n",
    "\n",
    "<br></br>\n",
    "Training alogorithm:\n",
    "- Store all the data\n",
    "\n",
    "<br></br>\n",
    "Prediction alogorithm:\n",
    "1. Calculate the distance from x to all points in the data being used\n",
    "2. Sort the points by increasing distance from x\n",
    "3. Predict the majority label (class) of the closest 'K' points\n",
    "\n",
    "<img src='pics/knn_and_post.png'>\n",
    "\n",
    "<br></br>\n",
    "- For low values of 'K' there may be a lot of noise\n",
    "- For high values of 'K' there will be less noise but the bias will be increased\n",
    "\n",
    "<img src='pics/knn_increase.png'>\n",
    "\n",
    "<br></br>\n",
    "Pros:\n",
    "- Easy to set up\n",
    "- Works with any number of classes\n",
    "\n",
    "<br></br>\n",
    "Cons:\n",
    "- High prediction cost (worse for large data sets)\n",
    "- Categorical features don't work well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

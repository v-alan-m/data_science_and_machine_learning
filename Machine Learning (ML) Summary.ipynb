{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "- Calculate the regression line by drawing a line that's as close to every data point as possible\n",
    "    - The **Least Sqaures Method** is used, and only measures the closeness in the **Up** and **Down** direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance and Cross Validation\n",
    "- Fundamental topic of understanding you models performance\n",
    "\n",
    "<br></br>\n",
    "## Bias-Variance\n",
    "- A point within the model where adding model complexity is only generated unwanted noise within the data\n",
    "- The **training error** goes down but the **test error** goes up\n",
    "- The model after the **bias-variance** trade-off begins to overfit\n",
    "\n",
    "<br></br>\n",
    "<img src='pics/bias-variance_trade-off.png'>\n",
    "\n",
    "<br></br>\n",
    "- If complexity is continually added to a model it will overfit to the training data\n",
    "    - Making it less usefull when applied to the untrained data, such as the test set\n",
    "\n",
    "<img src='pics/b_v_t_o_overfit.png'>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<img src='pics/b_v_flexibility.png'>\n",
    "\n",
    "<br></br>\n",
    "- **Flexibility:** Complexity of the model, i.e. the polynomial level of a regression fit (how many independent variables are being account for)\n",
    "- **Mean Squared Error**: The error metric\n",
    "\n",
    "<br></br>\n",
    "- The goal is to balance the **bias** and **variance** of your model, so that it has the lowest error\n",
    "    - Find training-data-point that closest to the mininium-error-point on the test-data\n",
    "    - Find the ideal number of variables for analysis\n",
    "        - In this example it is the blue dot, within the middle figure\n",
    "            - The blue dot is the quadratic fit\n",
    "\n",
    "<img src='pics/b_v_over_under_fit.png'>\n",
    "\n",
    "<br></br>\n",
    "- *Going left*: Higher bias, lower variance\n",
    "- *Going right*: Lower bias, higher variance\n",
    "- **Underfitting**: Not enough data has been included in the model, as the error could be reduced\n",
    "- **Overfitting**: Too much data has been included in the model, as the error could be reduced\n",
    "\n",
    "<br></br>\n",
    "- Pick a point as a **bias-variance trade-off**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "- Trying to predict the probability of discrete categories\n",
    "\n",
    "<br></br>\n",
    "- A method for classification\n",
    "    - Spam and real emails\n",
    "    - Loan default (yes/no)\n",
    "    - Diesease diagnosis (have/do not have)\n",
    "        - These are examples of **binary classification**, aka there are 2 classes\n",
    "            - Typically **0** and **1**\n",
    "            \n",
    "<br></br>\n",
    "- A linear regression will give a bad fit for **binary classification** models\n",
    "    - It would predict for values below zero\n",
    "        - Transofrmed into a **logistic regression** curve is used as it will only fit data betweem **0** and **1** \n",
    "\n",
    "<img src='pics/log_regress_curve.png'>\n",
    "\n",
    "<br></br>\n",
    "- The shape of the logistic regression curve is called the **Signoid**\n",
    "    - It will take in any value and only output a value between 0 and 1\n",
    "\n",
    "<img src='pics/log_reg_signoid.png'>\n",
    "\n",
    "<br></br>\n",
    "-  The *linear model* is transformed into the *logistic model*\n",
    "    - So that all the **output values** range from **0** to **1**\n",
    "\n",
    "<img src='pics/log_reg_transform.png'>\n",
    "\n",
    "<br></br>\n",
    "-  A line can be draw half way in the probability\n",
    "    - Anything **below** the line is **class 0**\n",
    "    - Anything **above** the line is **class 1**\n",
    "\n",
    "<img src='pics/log_reg_class_line.png'>\n",
    "\n",
    "<br></br>\n",
    "- Train the logistics model then test it\n",
    "    - A **confusion matrix** can be used to evaluate classification models\n",
    "\n",
    "<img src='pics/log_reg_confusion.png'>\n",
    "\n",
    "**Basic terminolgy:**\n",
    "- True positives (TP)\n",
    "- True negatives (TN)\n",
    "- False positives (FP): **Type 1 error**\n",
    "- True negatives (FN): **Type 2 error**\n",
    "\n",
    "<br></br>\n",
    "**Accuracy rate:**\n",
    "- How often is the value correct\n",
    "    - (TP + TN) / total = 150 / 165 = 0.91 = 91%\n",
    "\n",
    "<br></br>\n",
    "**Misclassification rate:**\n",
    "- How often is the value wrong\n",
    "    - (FP + FN) / total = 15 / 165 = 0.09 = 9%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbours (KNN)\n",
    "- Use for classification problems\n",
    "- When two or more classes have continuous parameters\n",
    "    - E.g. The hieght and weight of dogs and horses\n",
    "\n",
    "<img src='pics/knn_and_pre.png'>\n",
    "\n",
    "- Predict whether a data point represents a horse or a dog based on the height and weight\n",
    "\n",
    "<br></br>\n",
    "Training alogorithm:\n",
    "- Store all the data\n",
    "\n",
    "<br></br>\n",
    "Prediction alogorithm:\n",
    "1. Calculate the distance from x to all points in the data being used\n",
    "2. Sort the points by increasing distance from x\n",
    "3. Predict the majority label (class) of the closest 'K' points\n",
    "\n",
    "<img src='pics/knn_and_post.png'>\n",
    "\n",
    "<br></br>\n",
    "- For low values of 'K' there may be a lot of noise\n",
    "- For high values of 'K' there will be less noise but the bias will be increased\n",
    "\n",
    "<img src='pics/knn_increase.png'>\n",
    "\n",
    "<br></br>\n",
    "Pros:\n",
    "- Easy to set up\n",
    "- Works with any number of classes\n",
    "- Easy to add more data\n",
    "\n",
    "<br></br>\n",
    "Cons:\n",
    "- High prediction cost (worse for large data sets)\n",
    "- Categorical features don't work well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests\n",
    "## Decision trees\n",
    "- Make decision when there are different outcomes for many features\n",
    "\n",
    "Will my friend play (Yes/No) based on the weather\n",
    "\n",
    "<img src='pics/decision_tree.png'>\n",
    "\n",
    "- **Nodes**: Split value of a certain attribute (white box)\n",
    "- **Edges**: Connection between the nodes, denoting the outcome of splitting to a node\n",
    "- **Root**: The node that perform the first split\n",
    "- **Leaves**: Terminal nodes that predict the outcome (Red and green)\n",
    "\n",
    "<br></br>\n",
    "- Trying to choose the features that best split your data\n",
    "    - I.e. When the node is split the maximum number of each class should be on either side\n",
    "    - Referred to as *maximising the information gain* off of this split\n",
    "\n",
    "<br></br>\n",
    "## Random forests\n",
    "- Improve performance off single decision trees\n",
    "    - Use many trees with a random sample of features chosen as the split\n",
    "\n",
    "<br></br>\n",
    "- Sometimes the predictive accuracy of decision trees can be low due to the high variance\n",
    "    - Caused by the different splits in the training-data, which can lead to very different trees\n",
    "        - **Bagging** is method used to reduce the variance in machine learning models\n",
    "\n",
    "<br></br>\n",
    "- **Random forests** are a slight variation on bagged-decisions-trees with better performance \n",
    "- A new random sample of features is chosen for **every single tree at every single split**\n",
    "- Sampling from the training data set with replacement\n",
    "    - For **classification**, m is typically chosen to be the square root of **p**\n",
    "        - **m**: A random sample of features\n",
    "        - **p**: The full set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means Clustering\n",
    "- An unsupervised learning algorithm that will label unlabelled data\n",
    "    - Will attempt to group similar clusters of data together\n",
    "         - Cluster similar documents\n",
    "         - Cluster customer based on Features\n",
    "         - Market segmentation\n",
    "         - Identify similar physical groups\n",
    "         \n",
    "<br></br>\n",
    "- The goal is to divide data into distinct groups such that observations within each group are similar\n",
    "\n",
    "<img src='pics/k_means_cluster.png'>\n",
    "<br></br>\n",
    "- Unlaballed training data of the left\n",
    "- KMC algorithm trying to clsuter the data into 5 coloured groups\n",
    "\n",
    "<img src='pics/k_means_algo.png'>\n",
    "\n",
    "<img src='pics/k_means_cluster_process.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "- An unsupervised statistical technique used to examine the interrelations among a set of variables\n",
    "    - In order to identify the underlying structure to those variables\n",
    "        - Sometimes referred to as **factor analysis**\n",
    "        \n",
    "<br></br>\n",
    "- **Regresion** determines a line of best fit to a data set\n",
    "- **Factor analysis** determines several orthogonal lines of best fit to the data set\n",
    "    - The lines are perpendicular to each other in n-dimensional space\n",
    "        - 2 variables, 2 lines, 2D\n",
    "        - 3 variables, 3 lines, 3D\n",
    "- n-Dimensional Space is the variable sample space\n",
    "    - There are as many dimensions as there are variables\n",
    "    \n",
    "<img src='pics/principle_c_a.png'>\n",
    "\n",
    "<br></br>\n",
    "- The **componenets** are **linear transformations**, that chooses a variable from the data set\n",
    "    - The greatest variance of the data set comes to lie on the first axis\n",
    "    - The second greatest variance on the second axis\n",
    "\n",
    "<br></br>\n",
    "- The **components are uncorrelated** since they are orthogonal to each other in the sample space\n",
    "\n",
    "<br></br>\n",
    "- This process allows us to **reduce the number of variables** used in an analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems \n",
    "Two most common types of recommender systems:\n",
    "- **Content-based**\n",
    "    - Recommendations based on the attributes of the item (most similar items)\n",
    "- **Collaborative filtering (CF)**\n",
    "    - Recommendation based on the knowledge of the users' behaviour (crowd behaviour ) \n",
    "    \n",
    "<br></br>\n",
    "- Collaborative filtering (CF) can be divided into:\n",
    "    - **Memory-Based collaborative filtering**\n",
    "        - Compute cosine similarity\n",
    "    - **Model-Based collaborative filter**\n",
    "        - Used singluar value decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
